{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d791ed98",
   "metadata": {},
   "source": [
    "# 관광데이터 카테고리 자동 분류\n",
    "- 데이터로 주어지는 POI를 사용하여 관광지 카테고리 (대>중>소) 중 '소' 카테고리 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e1f4f1",
   "metadata": {},
   "source": [
    "## 1. Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c28d0f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install pytorch torchvision torchaudio cpuonly -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2caedd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73403f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install albumentations --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "698a3fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d251a05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7fdc3fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8157266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5408f512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A # fast image agumentation library\n",
    "from albumentations.pytorch.transforms import ToTensorV2 # 이미지 형 변환\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2b969c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6cb748c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "661618c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpu 사용\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c148e48a",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "761ee5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 사이즈, 에포크, 학습률, 배치 사이즈, 시드 -> 고정\n",
    "CFG = {\n",
    "    'IMG_SIZE': 128,\n",
    "    'EPOCHS': 5,\n",
    "    'LEARNING_RATE': 3e-4,\n",
    "    'BATCH_SIZE': 64,\n",
    "    'SEED': 41\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26286f07",
   "metadata": {},
   "source": [
    "## 3. Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f731bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED'])  # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e71a6e8",
   "metadata": {},
   "source": [
    "## 4. Data Load & Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ed8bb56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_csv('./train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c63d6550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set, validation set 구별\n",
    "train_df, val_df, _, _ = train_test_split(all_df, all_df['cat3'], test_size=0.2, random_state=CFG['SEED'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266a5768",
   "metadata": {},
   "source": [
    "## 5. Label - Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "32da8c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 카테고리형 데이터를 수치형으로 변환하는 labelencoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_df['cat3'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ce08acdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat3에 labelencoder 적용\n",
    "train_df['cat3'] = le.transform(train_df['cat3'].values)\n",
    "val_df['cat3'] = le.transform(val_df['cat3'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa2940f",
   "metadata": {},
   "source": [
    "## 6. Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "279b0f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview를 vectorize하는 vectorizer 선언, 최대 특성 수는 4096\n",
    "vectorizer = CountVectorizer(max_features=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c1ca01bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = vectorizer.fit_transform(train_df['overview'])\n",
    "train_vectors = train_vectors.todense()\n",
    "\n",
    "val_vectors = vectorizer.transform(val_df['overview'])\n",
    "val_vectors = val_vectors.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8cb0e698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13588, 4096)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4254a3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3398, 4096)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25f903b",
   "metadata": {},
   "source": [
    "## 7. Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "37e11f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 생성\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_path_list, text_vectors, label_list, transforms, infer=False):\n",
    "        self.img_path_list = img_path_list\n",
    "        self.text_vectors = text_vectors\n",
    "        self.label_list = label_list\n",
    "        self.transforms = transforms\n",
    "        self.infer = infer\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # NLP\n",
    "        text_vector = self.text_vectors[index]\n",
    "        \n",
    "        # Image 읽기\n",
    "        img_path = self.img_path_list[index]\n",
    "        image = cv2.imread(img_path)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image=image)['image'] # transforms(=image augmentation) 적용\n",
    "        \n",
    "        # Label\n",
    "        if self.infer: # infer == True, test_data로부터 label \"결과 추출\" 시 사용\n",
    "            return image, torch.Tensor(text_vector).view(-1)\n",
    "        else: # infer == False\n",
    "            label = self.label_list[index] # dataframe에서 label 가져와 \"학습\" 시 사용\n",
    "            return image, torch.Tensor(text_vector).view(-1), label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "49d7ddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "                            A.Resize(CFG['IMG_SIZE'],CFG['IMG_SIZE']),\n",
    "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "                            ToTensorV2()\n",
    "                            ])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "                            A.Resize(CFG['IMG_SIZE'],CFG['IMG_SIZE']),\n",
    "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "                            ToTensorV2()\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfba6ee",
   "metadata": {},
   "source": [
    "- albumentations: fast image augmentation library\n",
    "- albumentations.Compose: transform = A.Compose([])을 이용하여 이미지와 라벨 각각에 Augmentation을 적용하기 위한 객체 생성\n",
    "- albumentations.Resize(128, 128): 128 * 128 size로 resize\n",
    "- albumentations.Normalise(): 입력 받은 이미지 값의 범위를 (0, 255) -> (-1, 1) 범위로 줄여주는 역할\n",
    "- ToTensorV2: tensor형 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "11407e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# __init__(self, img_path_list, text_vectors, label_list, transforms, infer=False)\n",
    "train_dataset = CustomDataset(train_df['img_path'].values, train_vectors, train_df['cat3'].values, train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0) # 6\n",
    "\n",
    "val_dataset = CustomDataset(val_df['img_path'].values, val_vectors, val_df['cat3'].values, test_transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0) # 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164c39c0",
   "metadata": {},
   "source": [
    "## 8. Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "928d84c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, num_classes=len(le.classes_)):\n",
    "        super(CustomModel, self).__init__()\n",
    "        # Image\n",
    "        self.cnn_extract = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1),\n",
    "                # input_channel = 3 : RGB 3개의 채널이기 때문\n",
    "                # out_channel = 8 : 출력하는 채널 8개\n",
    "                # stride = 1 : stride 만큼 이미지 이동하면서 cnn 수행\n",
    "                # padding = 1 : zero-padding할 사이즈\n",
    "            nn.ReLU(), # 사용할 활성화 함수: Relu를 사용\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # 최댓값을 뽑아내는 맥스 풀링\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Text\n",
    "        self.nlp_extract = nn.Sequential(\n",
    "            nn.Linear(4096, 2048), # 선형회귀. 4096개의 입력으로 2048개의 출력\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 1024), # 선형회귀. 2048개의 입력으로 1024개의 출력\n",
    "        )\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(4160, num_classes)\n",
    "            # 선형회귀. 4160개의 입력으로 num_classes, 즉 cat3의 종류 개수만큼의 출력\n",
    "            # 근데 왜 4160개? \"4160 - 1024 = 3136\"이고 \"3136 / 64 = 49\". 즉 이미지는 \"7*7*64\"로 출력됨.\n",
    "        )\n",
    "    \n",
    "    def forward(self, img, text):\n",
    "        img_feature = self.cnn_extract(img) # cnn_extract 적용\n",
    "        img_feature = torch.flatten(img_feature, start_dim=1) # 1차원으로 변환\n",
    "        text_feature = self.nlp_extract(text) # nlp_extract 적용\n",
    "        feature = torch.cat([img_feature, text_feature], axis=1) # 2개 연결(3136 + 1024)\n",
    "        output = self.classifier(feature) # classifier 적용\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed86d267",
   "metadata": {},
   "source": [
    "결론\n",
    "- Image: conv -> ReLu -> MaxPooling -> conv -> ReLu -> MaxPooling -> conv -> ReLu -> MaxPooling -> conv -> ReLu -> MaxPooling ->\n",
    "- Text: Linear -> ReLu -> Linear\n",
    "- Classifier: Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae86ed5f",
   "metadata": {},
   "source": [
    "## 9. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0f0ed420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    model.to(device) # gpu(cpu)에 적용\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device) # CrossEntropyLoss: 다중분류를 위한 손실함수\n",
    "    best_score = 0\n",
    "    best_model = None # 최고의 모델을 추출하기 위한 파라미터\n",
    "    \n",
    "    for epoch in range(1,CFG[\"EPOCHS\"]+1):\n",
    "        model.train() # 학습시킴.\n",
    "        train_loss = []\n",
    "        for img, text, label in tqdm(iter(train_loader)): # train_loader에서 img, text, label 가져옴\n",
    "            img = img.float().to(device)\n",
    "            text = text.to(device)\n",
    "            label = label.type(torch.LongTensor) # label type을 LongTensor로 형변환, 추가하여 에러 해결\n",
    "            label = label.to(device)\n",
    "            \n",
    "            optimizer.zero_grad() # 이전 루프에서 .grad에 저장된 값이 다음 루프의 업데이트에도 간섭하는 걸 방지, 0으로 초기화\n",
    "\n",
    "            model_pred = model(img, text) # 예측\n",
    "            \n",
    "            loss = criterion(model_pred, label) # 예측값과 실제값과의 손실 계산\n",
    "\n",
    "            loss.backward() # .backward() 를 호출하면 역전파가 시작\n",
    "            optimizer.step() # optimizer.step()을 호출하여 역전파 단계에서 수집된 변화도로 매개변수를 조정\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        # 모든 train_loss 가져옴\n",
    "        tr_loss = np.mean(train_loss)\n",
    "            \n",
    "        val_loss, val_score = validation(model, criterion, val_loader, device) # 검증 시작, 여기서 validation 함수 사용\n",
    "            \n",
    "        print(f'Epoch [{epoch}], Train Loss : [{tr_loss:.5f}] Val Loss : [{val_loss:.5f}] Val Score : [{val_score:.5f}]')\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            # scheduler의 의미: Learning Rate Scheduler => learning rate를 조절한다. \n",
    "            # DACON에서는 CosineAnnealingLR 또는 CosineAnnealingWarmRestarts 를 주로 사용한다.\n",
    "            \n",
    "        if best_score < val_score: # 최고의 val_score을 가진 모델에 대해서만 최종적용을 시킴\n",
    "            best_score = val_score\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model # val_score가 가장 높은 모델을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "47aaf7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_function(real, pred):\n",
    "    return f1_score(real, pred, average=\"weighted\")\n",
    "\n",
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval() # nn.Module에서 train time과 eval time에서 수행하는 다른 작업을 수행할 수 있도록 switching 하는 함수\n",
    "    \n",
    "    model_preds = [] # 예측값\n",
    "    true_labels = [] # 실제값\n",
    "    \n",
    "    val_loss = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, text, label in tqdm(iter(val_loader)): # val_loader에서 img, text, label 가져옴\n",
    "            img = img.float().to(device)\n",
    "            text = text.to(device)\n",
    "            label = label.type(torch.LongTensor) # label type을 LongTensor로 형변환, 추가하여 에러 해결\n",
    "            label = label.to(device)\n",
    "            \n",
    "            model_pred = model(img, text)\n",
    "            \n",
    "            loss = criterion(model_pred, label) # 예측값, 실제값으로 손실함수 적용 -> loss 추출\n",
    "            \n",
    "            val_loss.append(loss.item()) # loss 출력, val_loss에 저장\n",
    "            \n",
    "            model_preds += model_pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "            true_labels += label.detach().cpu().numpy().tolist()\n",
    "        \n",
    "    test_weighted_f1 = score_function(true_labels, model_preds) # 실제 라벨값들과 예측한 라벨값들에 대해 f1 점수 계산\n",
    "    return np.mean(val_loss), test_weighted_f1 # 각각 val_loss, val_score에 적용됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdacf86",
   "metadata": {},
   "source": [
    "## 10. Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d606cae8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6393d7b70280490a8efe1d9ab17cd559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c95312cd23b4861a89f0bb269484e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [2.61566] Val Loss : [1.84804] Val Score : [0.49643]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "824f69bcbe3240aa9c72c3a035f263a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c83bf752fd4ef6801801e6ec2a8aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [1.35021] Val Loss : [1.54920] Val Score : [0.58578]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e142e37574a343baae1755ce8fb13372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ecffb80b2f04273b213be2d5f741aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.75017] Val Loss : [1.67538] Val Score : [0.60762]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91400dece2864618a5ae41bea6d98e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b335db6e824ddca0000fa054f2ef47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.37207] Val Loss : [1.92895] Val Score : [0.60951]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e25a6bef47c4a1a984fb2d2a20a75ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "debe4783b18245d4a2d82cae9c84e15e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Train Loss : [0.15506] Val Loss : [2.12751] Val Score : [0.61455]\n"
     ]
    }
   ],
   "source": [
    "model = CustomModel()\n",
    "model.eval()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "scheduler = None\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da62b25",
   "metadata": {},
   "source": [
    "## 11. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "04592465",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./test.csv')\n",
    "test_vectors = vectorizer.transform(test_df['overview'])\n",
    "test_vectors = test_vectors.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1636c708",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test_df['img_path'].values, test_vectors, None, test_transform, True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1ac33e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    model_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, text in tqdm(iter(test_loader)):\n",
    "            img = img.float().to(device)\n",
    "            text = text.to(device)\n",
    "            \n",
    "            model_pred = model(img, text)\n",
    "            model_preds += model_pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "    return model_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d7de6967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f889efab85b74fa3ba80c82f83fe6c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = inference(infer_model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9963aca",
   "metadata": {},
   "source": [
    "## 12. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "105c8538",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit['cat3'] = le.inverse_transform(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d1fde62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('./result/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
